# -*- coding: utf-8 -*-
"""ML_Project_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12G-7D0tjOwOvBUdF-i4_Lt-Ksh3lgKS3
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install contractions
import numpy as np 
import pandas as pd 
import os
import matplotlib.pyplot as plt
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer
import seaborn as sns
import unicodedata
import contractions
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score,KFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import roc_curve
from sklearn.model_selection import learning_curve
import warnings
import pickle
warnings.filterwarnings('ignore')
# %matplotlib inline

"""**DATA ACQUISITION**"""

! pip install kaggle

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download "kazanova/sentiment140"

! unzip "sentiment140"

depressed_1 = pd.read_csv('Suicide.csv',encoding='ISO-8859-1')
depressed_2 = pd.read_csv('Lonely.csv',encoding='ISO-8859-1')
depressed_3 = pd.read_csv('Hopeless.csv',encoding='ISO-8859-1')
depressed_4 = pd.read_csv('Depression.csv',encoding='ISO-8859-1')
depressed_5 = pd.read_csv('Depressed.csv',encoding='ISO-8859-1')
depressed_6 = pd.read_csv('Antidepressant.csv',encoding='ISO-8859-1')

col_names = ['target', 'id', 'date', 'flag', 'user', 'text']
non_depressed_1 = pd.read_csv('training.1600000.processed.noemoticon.csv',encoding='ISO-8859-1',names=col_names)

depressed_1.drop(['conversation_id', 'created_at', 'date', 'time', 'timezone', 'user_id', 'username', 'name', 'place', 'mentions', 'urls', 'photos', 'replies_count', 'retweets_count', 'likes_count', 'hashtags', 
      'cashtags', 'link', 'retweet', 'quote_url', 'video', 'user_rt_id', 'near', 'geo'], axis = 1,inplace = True)
depressed_2.drop(['conversation_id', 'created_at', 'date', 'time', 'timezone', 'user_id', 'username', 'name', 'place', 'mentions', 'urls', 'photos', 'replies_count', 'retweets_count', 'likes_count', 'hashtags', 
      'cashtags', 'link', 'retweet', 'quote_url', 'video', 'user_rt_id', 'near', 'geo'], axis = 1,inplace = True)
depressed_3.drop(['conversation_id', 'created_at', 'date', 'time', 'timezone', 'user_id', 'username', 'name', 'place', 'mentions', 'urls', 'photos', 'replies_count', 'retweets_count', 'likes_count', 'hashtags', 
      'cashtags', 'link', 'retweet', 'quote_url', 'video', 'user_rt_id', 'near', 'geo'], axis = 1,inplace = True)
depressed_4.drop(['time', 'hashtags', 'cashtags', 'Unnamed: 0'], axis = 1,inplace = True)
depressed_5.drop(['conversation_id', 'created_at', 'date', 'time', 'timezone', 'user_id', 'username', 'name', 'place', 'mentions', 'urls', 'photos', 'replies_count', 'retweets_count', 'likes_count', 'hashtags', 
      'cashtags', 'link', 'retweet', 'quote_url', 'video', 'user_rt_id', 'near', 'geo'], axis = 1,inplace = True)
depressed_6.drop(['conversation_id', 'created_at', 'date', 'time', 'timezone', 'user_id', 'username', 'name', 'place', 'mentions', 'urls', 'photos', 'replies_count', 'retweets_count', 'likes_count', 'hashtags', 
      'cashtags', 'link', 'retweet', 'quote_url', 'video', 'user_rt_id', 'near', 'geo'], axis = 1,inplace = True)
non_depressed_1.drop(['date', 'flag', 'user'], axis = 1,inplace = True)

depressed_1.shape, depressed_2.shape, depressed_3.shape, depressed_4.shape, depressed_5.shape, depressed_6.shape

df1 = depressed_1.copy().sample(2000, random_state=42)
df1["label"] = 1
df2 = depressed_2.copy().sample(2000, random_state=42)
df2["label"] = 1
df3 = depressed_3.copy().sample(2000, random_state=42)
df3["label"] = 1
df4 = depressed_4.copy().sample(2000, random_state=42)
df4["label"] = 1
df5 = depressed_5.copy().sample(2000, random_state=42)
df5["label"] = 1
df6 = depressed_6.copy()
df6["label"] = 1

non_depressed_1 = non_depressed_1[non_depressed_1['target']==4]
df7 = non_depressed_1.copy().sample(8000, random_state=42)
df7["label"] = 0
df7 = df7[['id','text', 'label']]
df7.rename(columns = {'text':'tweet'},inplace=True)

frames = [df1, df2, df3, df4, df5, df6, df7]
final_data = pd.concat(frames)
final_data = final_data.sample(frac = 1, random_state = 42)
final_data.reset_index(inplace=True)
final_data.drop(['index'],axis=1,inplace=True)
display(final_data)

final_data.to_csv('ML_Dataset.csv',index=False)

"""**PREPROCESSING**"""

print("Dimensions of our Dataset: ", final_data.shape)
print("\n")

print("Datset Info: \n")
final_data.info()

print("\033[1m" + "Null Values" + "\033[0m\n")
print(final_data.isnull().sum(), "\n")
print("\033[1m" + "______________________________________________________________________________" + "\033[0m\n")
print("\033[1m" + "Duplicate Values" + "\033[0m\n")
display(final_data[final_data.duplicated(keep=False)])
final_data.drop_duplicates(inplace = True)
final_data.reset_index(inplace=True)
final_data.drop(['index'],axis=1,inplace=True)
print("\n\033[1m" + "New DataSet Shape" + "\033[0m\n")
final_data.shape

tweet_length = final_data["tweet"].apply(len)
plt.figure(figsize=(10,5))
sns.histplot(tweet_length, color='pink')
plt.title("Distribution of Lengths of all Tweets")
plt.show()

Counts=final_data.label.value_counts()
Counts.rename(index={0: 'Non-Depressive', 1: 'Depressive'}, inplace=True)
print("Number of tweets for each depressive and non depressive category: \n", Counts)
print("\n")
Counts.plot(kind='bar', figsize=(8,8), ylabel='Number of posts', xlabel='Category', title='Bar graph for types of tweets', color='yellow', edgecolor = 'black')
plt.xticks(rotation=360)
plt.show()

final_data.drop(['id'], axis=1, inplace = True)
data_before_preprocessing = final_data.copy(deep=True)

# Removing URL links from texts.
for i in range(final_data.shape[0]):
  processed_tweet_1 = re.sub(r"http\S+",' ', final_data._get_value(i, 'tweet'))
  final_data._set_value(i, 'tweet', processed_tweet_1)

# Removing non-ascii-characters.
for i in range(final_data.shape[0]):
  t = final_data._get_value(i, 'tweet')
  t = [w for w in t.split(' ')]
  new_words = []
  for word in t:
          new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')
          new_words.append(new_word)
  t = " ".join(new_words)
  final_data._set_value(i, 'tweet', t)

# Expanding Contractions
for i in range(final_data.shape[0]):
  tweet = final_data._get_value(i, 'tweet')
  expanded_tweet = contractions.fix(tweet)
  final_data._set_value(i, 'tweet', expanded_tweet)

# Converting to lower case
final_data["tweet"] = final_data["tweet"].str.lower()

# Removing @ mentions from the tweets.
for i in range(final_data.shape[0]):
  tweet = final_data._get_value(i, 'tweet')
  clean_tweet = re.sub("@[A-Za-z0-9_]+","", tweet)
  final_data._set_value(i, 'tweet', clean_tweet)

#removing special characters and numbers from texts.
for i in range(final_data.shape[0]):
  t = final_data._get_value(i, 'tweet')
  p = re.compile('\W+')
  t = re.sub(p, ' ', t)
  p = re.compile(r'[0-9]')
  t = re.sub(p, ' ', t)
  p = re.compile(r'[_+]')
  t = re.sub(p, ' ', t)
  final_data._set_value(i, 'tweet',t)

# Removing extra space
for i in range(final_data.shape[0]):
  t = final_data._get_value(i, 'tweet')
  p = re.compile('\s+')
  t = re.sub(p, ' ', t)
  final_data._set_value(i, 'tweet', t)

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

# remove stop words
remove_words = stopwords.words("english")
for i in range(final_data.shape[0]):
  t = final_data._get_value(i, 'tweet')
  t = " ".join([w for w in t.split(' ') if w not in remove_words])
  final_data._set_value(i, 'tweet', t)

#Lemmatization (grouping similar words)
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
nltk.download('wordnet')
for i in range(final_data.shape[0]):
  t = final_data._get_value(i, 'tweet')
  t = " ".join([lemmatizer.lemmatize(w) for w in t.split(' ')])
  final_data._set_value(i, 'tweet', t)

print("Text before preprocessing :", data_before_preprocessing.iloc[89,0], "\n")
print("Text after preprocessing :", final_data.iloc[89,0], "\n")

def imp_words(relevant_data, data):
    vec = TfidfVectorizer().fit(data)
    bag_of_words = vec.transform(relevant_data)
    words_sum = bag_of_words.sum(axis=0) 
    words_freq = [(word, words_sum[0, idx]) for word, idx in vec.vocabulary_.items()]
    imp_words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)
    imp_words_freq = imp_words_freq[:20]
    return imp_words_freq

depressive_data = final_data[final_data['label'] == 1]
imp_depressive_words = imp_words(depressive_data['tweet'], final_data['tweet'])

plt.figure(figsize=(30,9))
plt.subplot(1, 2, 1)
words = [x[0] for x in imp_depressive_words]
imp = [x[1] for x in imp_depressive_words]
sns.barplot(x = imp, y = words)
plt.xlabel('Importance - tfidf value')
plt.title('Important Words in Depressive Tweets')

non_depressive_data = final_data[final_data['label'] == 0]
imp_non_depressive_words = imp_words(non_depressive_data['tweet'], final_data['tweet'])

plt.subplot(1, 2, 2)
words = [x[0] for x in imp_non_depressive_words]
imp = [x[1] for x in imp_non_depressive_words]
sns.barplot(x = imp, y = words)
plt.xlabel('Importance - tfidf value')
plt.title('Important Words in Non-Depressive Tweets')
plt.savefig('bar.png')
plt.show()

depressive_data= final_data[final_data['label']==1]
tweets = " ".join(depressive_data['tweet'].tolist())
wordcount=WordCloud(background_color="black", max_words=200, contour_width=3, stopwords=remove_words)
wordcount.generate(tweets)
plt.figure(figsize=(30,10))
plt.subplot(1, 2, 1)
plt.axis("off")
plt.title("Depressive Tweet's WordCloud")
plt.imshow(wordcount, interpolation='bilinear')

print()

non_depressive_data= final_data[final_data['label']==0]
tweets = " ".join(non_depressive_data['tweet'].tolist())
wordcount=WordCloud(background_color="white", max_words=200, contour_width=3, stopwords=remove_words)
wordcount.generate(tweets)
plt.subplot(1, 2, 2)
plt.axis("off")
plt.title("Non-Depressive Tweet's WordCloud")
plt.imshow(wordcount, interpolation='bilinear')
plt.savefig('word_cloud.png')
plt.show()

"""**Feature Extraction**"""

frequency = final_data.tweet.str.split(expand = True).stack().value_counts()
relevant_words = frequency[frequency.values >= 10]
relevant_words

tweet_list = final_data.tweet.to_list()
vec = CountVectorizer(stop_words='english', max_features= len(relevant_words))
features = vec.fit_transform(tweet_list)
transformer = TfidfTransformer()
final_features =  transformer.fit_transform(features).toarray()
X = pd.DataFrame(final_features)
y = final_data['label']

"""**Train - Test Split**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""**LEARNING TECHNIQUES**

**Baseline model -** LOGISTIC REGRESSION
"""

# Logistic regression
a = LogisticRegression(penalty="none", max_iter = 1000, C = 1, solver="lbfgs")
kf = KFold(n_splits=10, random_state = 42, shuffle = True)
score = cross_val_score(a, X_train, y_train, cv=kf, scoring='accuracy')
print("Cross Validation Scores are {}".format(score))
print("Average Cross Validation score :{}".format(score.mean()))

C = np.logspace(-3,3,7)
penalty = ["l1","l2"]
solver = ["liblinear"]
max_iter = [100, 500, 1000, 1500]

'''param_grid = dict(C=C, penalty = penalty, solver = solver, max_iter = max_iter)'''

param_grid = dict(C=C, penalty = penalty, solver = solver, max_iter = max_iter)
classifier = LogisticRegression(C=C, penalty = penalty, solver = solver, max_iter = max_iter)
grid = GridSearchCV(estimator=classifier, param_grid=param_grid, cv = 5)
results = grid.fit(X_train, y_train)

# Summarize the results in a readable format
print("Best: {0}, using {1}".format(results.cv_results_['mean_test_score'].max(), results.best_params_))
df_log_reg = pd.DataFrame(results.cv_results_)
df_log_reg

model = LogisticRegression(C = 10.0, max_iter = 100, penalty = 'l2', solver = 'liblinear')
kf = KFold(n_splits=10, random_state = 42, shuffle = True)
score = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy')
print("Cross Validation Scores are {}".format(score))
print("Average Cross Validation score :{}".format(score.mean()))

# Classification Report
print("\n\033[1m"+"Performance on Test Set" + "\033[0m\n")
model.fit(X_train, y_train)
predictions = model.predict(X_test)
print(classification_report(y_test, predictions))

# Confusion Matrix
print("\n")
plot_confusion_matrix(model, X_test, y_test)  
plt.title("Confusion matrix for Depressive (1)/ Non-Depressive (0) Tweets", size = 10);
plt.savefig('lg_confusion_m.png')
plt.show()

# AUC-ROC Curve
print("\n")
fpr = {}
tpr = {}
thresh = {}
labels = model.classes_
pred_y_prob = model.predict_proba(X_test)
for i in range(2):    
  fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_y_prob[:,i],pos_label=labels[i])
      
plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label= labels[0])
plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label= labels[1])

plt.title("AUC-ROC curve for Logistic Regression")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.show()

# Learning Curve
train_score = model.score(X_train, y_train)
train_size, train_scores, test_scores = learning_curve(estimator=model, X=X_train, y=y_train, cv=kf, scoring="accuracy", random_state=42)
test_scores = 1-np.mean(test_scores,axis=1)
train_scores = 1-np.mean(train_scores,axis=1)
lc = pd.DataFrame({"Training_size":train_size,"Training_loss":train_scores,"Validation_loss":test_scores}).melt(id_vars="Training_size")
sns.lineplot(x="Training_size", y="value", data=lc, hue="variable")
plt.title("Learning Curve")
plt.ylabel("Misclassification Rate/Loss")
plt.savefig('learning_curve.png')
plt.show()

pickle.dump(model, open("model_Logistic_reg.pkl", 'wb'))

data_before_preprocessing.rename(columns={'tweet': 'tweet before preprocessing'}, inplace = True)
error = pd.DataFrame({'Actual_Values': y_test, 'Predicted_Values': predictions})
frames = [data_before_preprocessing.loc[X_test.index, 'tweet before preprocessing'], final_data.loc[X_test.index, 'tweet'], error]
error = pd.concat(frames, axis = 1)
error_analyse = error[error["Actual_Values"]!=error["Predicted_Values"]]
error_analyse

correct = error[error["Actual_Values"]==error["Predicted_Values"]]
correct

"""**Advanced model -** RNN"""

!pip install keras-tuner
from tensorflow import keras
import keras_tuner as kt
from keras.callbacks import EarlyStopping
from keras.models import Sequential
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from keras.layers import Dropout
import tensorflow as tf

X_test = final_data.loc[X_test.index, 'tweet'].to_frame()
y_test = final_data.loc[y_test.index, 'label']
X_mix = final_data.loc[X_train.index, 'tweet'].to_frame()
y_mix = final_data.loc[y_train.index, 'label']
X_train, X_val, y_train, y_val = train_test_split(X_mix, y_mix, test_size = 0.1, random_state = 42)
X_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape

training_data = X_train
training_data["label"] = y_train

testing_data = X_test
testing_data["label"] = y_test

val_data = X_val
val_data["label"] = y_val

training_dataset = (tf.data.Dataset.from_tensor_slices((tf.cast(training_data['tweet'].values, tf.string),tf.cast(training_data['label'].values, tf.int32))))
testing_data = (tf.data.Dataset.from_tensor_slices((tf.cast(testing_data['tweet'].values, tf.string),tf.cast(testing_data['label'].values, tf.int32))))
val_data = (tf.data.Dataset.from_tensor_slices((tf.cast(val_data['tweet'].values, tf.string),tf.cast(val_data['label'].values, tf.int32))))

training_dataset = training_dataset.batch(32).prefetch(tf.data.AUTOTUNE)
testing_data = testing_data.batch(32).prefetch(tf.data.AUTOTUNE)
val_data = val_data.batch(32).prefetch(tf.data.AUTOTUNE)

VOCAB_SIZE = 1000
encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)
encoder.adapt(training_dataset.map(lambda tweet, label: tweet))

def model_builder(hp):
  hp_units = hp.Int('units', min_value=16, max_value=512, step=32)
  model = tf.keras.Sequential([encoder,
    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 32, mask_zero=True),
    tf.keras.layers.SimpleRNN(32),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(units=hp_units, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1, activation='sigmoid')
  ])
  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
  model.compile(loss=tf.keras.losses.BinaryCrossentropy(),
              optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),
              metrics=['accuracy'])
  return model

tuner = kt.Hyperband(model_builder,
                     objective='val_accuracy',
                     max_epochs=20,
                     factor=3,
                     directory='my_dir',
                     project_name='intro_to_kt_hppp')

es = EarlyStopping(monitor='val_loss', patience=3, verbose=1)
tuner.search(training_dataset, epochs=50, validation_data=val_data, validation_steps=20, callbacks = [es])

best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal number of units in the first densely-connected
layer is {best_hps.get('units')} and the optimal learning rate for the optimizer
is {best_hps.get('learning_rate')}.
""")

model = tuner.hypermodel.build(best_hps)
history = model.fit(training_dataset, epochs=20, validation_data=val_data, validation_steps=20)

val_acc_per_epoch = history.history['val_accuracy']
best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1
print('Best epoch: %d' % (best_epoch,))

def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric], '')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend(['Training_'+metric, 'Val_'+metric])
  plt.savefig('rnn_loss_curve.png')

plt.figure(figsize=(16, 6))
plt.subplot(1, 2, 1)
plot_graphs(history, 'accuracy')
plt.subplot(1, 2, 2)
plot_graphs(history, 'loss')

hypermodel = tuner.hypermodel.build(best_hps)
hypermodel.fit(training_dataset, epochs=best_epoch , validation_data=val_data, validation_steps=20)

pickle.dump(hypermodel, open("model_RNN.pkl", 'wb'))

eval_result = hypermodel.evaluate(training_dataset)
print("[train loss, train accuracy]:", eval_result)

eval_result = hypermodel.evaluate(testing_data)
print("[test loss, test accuracy]:", eval_result)

predictions = (hypermodel.predict(X_test['tweet']) > 0.5).astype("int32")

# Classification Report
print("\n\033[1m"+"Performance on Test Set" + "\033[0m\n")
print(classification_report(y_test, predictions))

# Confusion Matrix
print("\n")
cm = confusion_matrix(y_test, predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion matrix for Depressive (1)/ Non-Depressive (0) Tweets", size = 10);
plt.savefig('rnn_confusion_m.png')
plt.show()

# AUC-ROC Curve
print("\n")
pred_y_prob = hypermodel.predict(X_test['tweet']).ravel()   
fpr_rnn, tpr_rnn, thresh_rnn = roc_curve(y_test, pred_y_prob)
plt.plot(fpr_rnn, tpr_rnn, linestyle='--',color='orange', label="RNN")
plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label="Logistic Regression")

plt.title("AUC-ROC curve")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('AUC_ROC.png')
plt.show()

"""**Advanced model -** LSTM"""

def build_model2(hp):
    hp_units = hp.Int('units',min_value=32,max_value=512,step=32)
    hp_drop_rate = hp.Float('rate', min_value=0, max_value=0.9, step=0.1)
    hp_choice = hp.Choice('activation',values=['relu', 'sigmoid'])

    model = tf.keras.Sequential([encoder,
    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 32, mask_zero=True),
    tf.keras.layers.SpatialDropout1D(rate = hp_drop_rate),
     tf.keras.layers.LSTM(32),
    tf.keras.layers.Dropout(rate = hp_drop_rate),
    tf.keras.layers.Dense(units=hp_units, activation='relu'),
    tf.keras.layers.Dropout(rate = hp_drop_rate),
    tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),
              metrics=['accuracy'])
    return model

tuner = kt.Hyperband(build_model2,
                     objective='val_accuracy',
                     max_epochs=20,
                     factor=3,
                     directory='my_dir2',
                     project_name='intro_to_kt2')

es = EarlyStopping(monitor='val_loss', patience=3, verbose=1)
tuner.search(training_dataset, epochs=50, validation_data=val_data, validation_steps=20, callbacks = [es])

best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal number of units in the first densely-connected
layer is {best_hps.get('units')} and the optimal learning rate for the optimizer
is {best_hps.get('learning_rate')}.
""")

model = tuner.hypermodel.build(best_hps)
history = model.fit(training_dataset, epochs=20, validation_data=val_data, validation_steps=20)

val_acc_per_epoch = history.history['val_accuracy']
best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1
print('Best epoch: %d' % (best_epoch,))

def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric], '')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend(['Training_'+metric, 'Val_'+metric])
  plt.savefig('lstm_loss_curve.png')

plt.figure(figsize=(16, 6))
plt.subplot(1, 2, 1)
plot_graphs(history, 'accuracy')
plt.subplot(1, 2, 2)
plot_graphs(history, 'loss')

hypermodel = tuner.hypermodel.build(best_hps)
hypermodel.fit(training_dataset, epochs=best_epoch , validation_data=val_data, validation_steps=20)

pickle.dump(hypermodel, open("model_LSTM.pkl", 'wb'))

eval_result = hypermodel.evaluate(training_dataset)
print("[train loss, train accuracy]:", eval_result)

eval_result = hypermodel.evaluate(testing_data)
print("[test loss, test accuracy]:", eval_result)

eval_result = hypermodel.evaluate(val_data)
print("[validation loss, validation accuracy]:", eval_result)

predictions = (hypermodel.predict(X_test['tweet']) > 0.5).astype("int32")

# Classification Report
print("\n\033[1m"+"Performance on Test Set" + "\033[0m\n")
print(classification_report(y_test, predictions))

# Confusion Matrix
print("\n")
cm = confusion_matrix(y_test, predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion matrix for Depressive (1)/ Non-Depressive (0) Tweets", size = 10);
plt.savefig('rnn_confusion_m.png')
plt.show()

# AUC-ROC Curve
print("\n")
pred_y_prob = hypermodel.predict(X_test['tweet']).ravel()   
fpr_lstm, tpr_lstm, thresh_lstm = roc_curve(y_test, pred_y_prob)
plt.plot(fpr_lstm, tpr_lstm, linestyle='--',color='orange', label="LSTM")
plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label="Logistic Regression")

plt.title("AUC-ROC curve")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('AUC_ROC.png')
plt.show()

"""**Advanced model -** CNN"""

def model_builder(hp):
  hp_units = hp.Int('units', min_value=16, max_value=512, step=32)
  model = tf.keras.Sequential([encoder,
    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 32, mask_zero=True),
    tf.keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu'),
    tf.keras.layers.MaxPooling1D(pool_size=2),
    tf.keras.layers.GlobalMaxPooling1D(),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(units=hp_units, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1, activation='sigmoid')
  ])
  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
  model.compile(loss=tf.keras.losses.BinaryCrossentropy(),
              optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),
              metrics=['accuracy'])
  return model
  
tuner = kt.Hyperband(model_builder,
                     objective='val_accuracy',
                     max_epochs=20,
                     factor=3,
                     directory='my_dir',
                     project_name='intro_to_kttp')

es = EarlyStopping(monitor='val_loss', patience=3, verbose=1)
tuner.search(training_dataset, epochs=50, validation_data=val_data, validation_steps=20, callbacks = [es])

best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal number of units in the first densely-connected
layer is {best_hps.get('units')} and the optimal learning rate for the optimizer
is {best_hps.get('learning_rate')}.
""")

model = tuner.hypermodel.build(best_hps)
history = model.fit(training_dataset, epochs=20, validation_data=val_data, validation_steps=20)

val_acc_per_epoch = history.history['val_accuracy']
best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1
print('Best epoch: %d' % (best_epoch,))

def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric], '')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend(['Training_'+metric, 'Val_'+metric])
  plt.savefig('cnn_loss_curve.png')

plt.figure(figsize=(16, 6))
plt.subplot(1, 2, 1)
plot_graphs(history, 'accuracy')
plt.subplot(1, 2, 2)
plot_graphs(history, 'loss')

hypermodel = tuner.hypermodel.build(best_hps)
hypermodel.fit(training_dataset, epochs=best_epoch , validation_data=val_data, validation_steps=20)

pickle.dump(hypermodel, open("model_CNN.pkl", 'wb'))

eval_result = hypermodel.evaluate(training_dataset)
print("[train loss, train accuracy]:", eval_result)

eval_result = hypermodel.evaluate(testing_data)
print("[test loss, test accuracy]:", eval_result)

predictions = (hypermodel.predict(X_test['tweet']) > 0.5).astype("int32")

# Classification Report
print("\n\033[1m"+"Performance on Test Set" + "\033[0m\n")
print(classification_report(y_test, predictions))

# Confusion Matrix
print("\n")
cm = confusion_matrix(y_test, predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion matrix for Depressive (1)/ Non-Depressive (0) Tweets", size = 10);
plt.savefig('cnn_confusion_m.png')
plt.show()

# AUC-ROC Curve
print("\n")
pred_y_prob = hypermodel.predict(X_test['tweet']).ravel()   
fpr_rnn, tpr_rnn, thresh_rnn = roc_curve(y_test, pred_y_prob)
plt.plot(fpr_rnn, tpr_rnn, linestyle='--',color='orange', label="CNN")
plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label="Logistic Regression")

plt.title("AUC-ROC curve")
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive rate')
plt.legend(loc='best')
plt.savefig('AUC_ROC.png')
plt.show()